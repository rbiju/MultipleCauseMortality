import os
import pickle
import numpy as np
import keras
from keras import Sequential
from keras.layers import Dense, LSTM, InputLayer, PReLU, Dropout
from keras.initializers import Constant
import matplotlib.pyplot as plt

# files generated by LSTM_dataPrep.py
train_data = np.load(os.getcwd() + '/train_data.npy')
train_labels = np.load(os.getcwd() + '/train_labels.npy')

test_data = np.load(os.getcwd() + '/test_data.npy')
test_labels = np.load(os.getcwd() + '/test_labels.npy')

with open('tokenizer.pickle', 'rb') as handle:
    tokenizer = pickle.load(handle)

vocabulary_size = len(tokenizer.word_counts) + 1
seq_len = train_data.shape[1]

# make data LSTM compatible
train_data = train_data.reshape((np.shape(train_data)[0], np.shape(train_data)[1], 1))
test_data = test_data.reshape((np.shape(test_data)[0], np.shape(test_data)[1], 1))

model = Sequential()

model.add(LSTM(50, input_shape=(seq_len, 1), return_sequences=True))
model.add(LSTM(50))
model.add(Dense(128))
model.add(Dropout(0.3))
model.add(PReLU(alpha_initializer=Constant(value=0.25)))
model.add(Dense(vocabulary_size, activation='softmax'))

model.summary()

opt = keras.optimizers.Adam(learning_rate=0.005)
model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
history = model.fit(train_data, train_labels, batch_size=1000, epochs=150,
                    validation_data=(test_data, test_labels), verbose=True)

#  "Accuracy"
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
# "Loss"
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

model.save('MSP_NLP.h5')
